{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c55846",
   "metadata": {},
   "source": [
    "Online Webstore Dataset Analysis\n",
    "\n",
    "In this notebook, you will find commands to read in 3 datasets that help an online webstore track customer sales behaviour.\n",
    "\n",
    "The 3 main datasets are:\n",
    "\n",
    "Customer: A dataset containing basic customer information <br>\n",
    "Product: A dataset containing information about specific products and their prices <br>\n",
    "Sales: A dataset containing information about purchases made by a customer <br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab182580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/11 12:53:38 WARN Utils: Your hostname, codespaces-22a5ca resolves to a loopback address: 127.0.0.1; using 10.0.12.164 instead (on interface eth0)\n",
      "25/06/11 12:53:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/11 12:53:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType, DateType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Spark Example\").getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the schema\n",
    "sales_schema = StructType([\n",
    "    StructField(\"SaleID\", IntegerType(), True),\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"ProductIDs\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"Purchase_Date\", StringType(), True)  # Using StringType for simplicity\n",
    "])\n",
    "\n",
    "\n",
    "sales = spark.read.parquet('sales_dataset.parquet', schema =sales_schema,header=True)\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e309a",
   "metadata": {},
   "source": [
    "<h5> Question 1: Based on how we have read in the schema for the sales dataset, can you apply a schema to the customer and sales dataset? </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d3d70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer = spark.read.parquet('customer_dataset.parquet', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eb46c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+------------+--------------------+\n",
      "|CustomerId|       Name|       DoB|Phone Number|               Email|\n",
      "+----------+-----------+----------+------------+--------------------+\n",
      "|       251|Customer251|1972-07-07| 35452398098|customer251@examp...|\n",
      "|       252|Customer252|1983-11-16| 53870960356|customer252@examp...|\n",
      "|       253|Customer253|1991-05-24| 04955017858|customer253@examp...|\n",
      "|       254|Customer254|1983-10-04| 59412216040|customer254@examp...|\n",
      "|       255|Customer255|1984-07-12| 00412449253|customer255@examp...|\n",
      "|       256|Customer256|1979-03-19| 40953435482|customer256@examp...|\n",
      "|       257|Customer257|1990-05-18| 01745444544|customer257@examp...|\n",
      "|       258|Customer258|1975-08-22| 14587131559|customer258@examp...|\n",
      "|       259|Customer259|1971-02-02| 43213131663|customer259@examp...|\n",
      "|       260|Customer260|1980-02-25| 78948060570|customer260@examp...|\n",
      "|       261|Customer261|1995-07-24| 28556492385|customer261@examp...|\n",
      "|       262|Customer262|1990-01-23| 71531684256|customer262@examp...|\n",
      "|       263|Customer263|1988-01-08| 43612167427|customer263@examp...|\n",
      "|       264|Customer264|1985-02-07| 15500380893|customer264@examp...|\n",
      "|       265|Customer265|1974-10-28| 60508742134|customer265@examp...|\n",
      "|       266|Customer266|1995-06-27| 76233701210|customer266@examp...|\n",
      "|       267|Customer267|1991-01-18| 14515974129|customer267@examp...|\n",
      "|       268|Customer268|1977-02-19| 79705875453|customer268@examp...|\n",
      "|       269|Customer269|1991-07-17| 86703310056|customer269@examp...|\n",
      "|       270|Customer270|1976-10-18| 01739551517|customer270@examp...|\n",
      "+----------+-----------+----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57715613",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = spark.read.parquet('product_dataset.parquet', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "941192ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+\n",
      "|ProductID|     Item| cost|\n",
      "+---------+---------+-----+\n",
      "|       51|Product51|96.34|\n",
      "|       52|Product52|32.98|\n",
      "|       53|Product53|25.57|\n",
      "|       54|Product54|50.34|\n",
      "|       55|Product55|60.44|\n",
      "|       56|Product56|56.05|\n",
      "|       57|Product57|60.02|\n",
      "|       58|Product58|20.39|\n",
      "|       59|Product59|86.19|\n",
      "|       60|Product60|70.27|\n",
      "|       61|Product61|92.14|\n",
      "|       62|Product62|59.97|\n",
      "|       63|Product63|94.48|\n",
      "|       64|Product64|17.87|\n",
      "|       65|Product65|63.52|\n",
      "|       66|Product66|97.33|\n",
      "|       67|Product67|98.65|\n",
      "|       68|Product68|13.61|\n",
      "|       69|Product69|44.98|\n",
      "|       70|Product70|11.55|\n",
      "+---------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba4dda9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "customer.createOrReplaceTempView(\"tmp_customer\")\n",
    "sales.createOrReplaceTempView(\"tmp_sales\")\n",
    "product.createOrReplaceTempView(\"tmp_product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14d66e",
   "metadata": {},
   "source": [
    "<h5> Question 2: Calculate the number of customers that have signed up to the website that have made a purchase on the website </h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d58a87",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mselect count(distinct c.customerid) from tmp_customer \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select count(distinct c.customerid) from tmp_customer \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b13202",
   "metadata": {},
   "source": [
    "<h5> Question 3: we want to find all the customers that haven't made a purchase yet as part of a marketing campaign, please write some code to generate that - Hint: Make use of the explode function </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "27df8f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         sum(cost)|\n",
      "+------------------+\n",
      "|225.35000000000002|\n",
      "| 661.8600000000001|\n",
      "|             474.5|\n",
      "|           1050.48|\n",
      "|            165.84|\n",
      "| 463.4699999999999|\n",
      "|            686.48|\n",
      "|            369.24|\n",
      "|349.71999999999997|\n",
      "| 706.6800000000001|\n",
      "|             263.5|\n",
      "|           1180.92|\n",
      "|443.34000000000003|\n",
      "| 94.32000000000001|\n",
      "|            476.64|\n",
      "|              94.4|\n",
      "|1645.4299999999998|\n",
      "| 918.9000000000001|\n",
      "|            104.37|\n",
      "|            758.88|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Here we have intentionally picked the first item in the array to give the wrong value, we are instead expecting them to try and explode this array and get the correct answer this way\n",
    "spark.sql('select sum(cost) from tmp_product as p join tmp_sales as s on element_at(s.productIDs, 1) = p.productID group by p.productID').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00164a48",
   "metadata": {},
   "source": [
    "<h5> Question 4: Please Generate the date that a customer made their second purchase on the website. Hint: Make use of the Row_Number function </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "577a72fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|CustomerID|FirstPurchase|\n",
      "+----------+-------------+\n",
      "|       001|   2023-10-23|\n",
      "|       001|   2023-10-23|\n",
      "|       001|   2023-10-23|\n",
      "|       001|   2023-10-23|\n",
      "|       002|   2023-11-16|\n",
      "|       002|   2023-11-16|\n",
      "|       003|   2023-10-15|\n",
      "|       003|   2023-10-15|\n",
      "|       003|   2023-10-15|\n",
      "|       003|   2023-10-15|\n",
      "|       004|   2023-08-12|\n",
      "|       004|   2023-08-12|\n",
      "|       004|   2023-08-12|\n",
      "|       005|   2023-12-30|\n",
      "|       005|   2023-12-30|\n",
      "|       005|   2023-12-30|\n",
      "|       005|   2023-12-30|\n",
      "|       007|   2023-08-11|\n",
      "|       007|   2023-08-11|\n",
      "|       007|   2023-08-11|\n",
      "+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select CustomerID, FIRST_VALUE(Purchase_Date) over (Partition By CustomerID Order By Purchase_Date Desc) as FirstPurchase from tmp_sales').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9842ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
