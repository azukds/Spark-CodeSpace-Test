{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5de7339",
   "metadata": {},
   "source": [
    "Generate Dummy Dataset with the following data inside:\n",
    "\n",
    "Contact Dataset\n",
    "CustomerId, Name, DoB, Phone Number, Email\n",
    "\n",
    "Sales Dataset\n",
    "SaleID, CustomerID, Array of ProductIDs, Purchase Date\n",
    "\n",
    "Product Dataset\n",
    "ProductID, Item, cost\n",
    "\n",
    "Potential Calculations:\n",
    "\n",
    "\n",
    "Calculate best selling item\n",
    "Highest Value Customer\n",
    "Date with most sales\n",
    "Day of week with most sales\n",
    "\n",
    "Ways of testing them:\n",
    "\n",
    "We don't use schemas for all of the datasets, intentially don't do it for one where it will cause an issue e.g. the contact dataset\n",
    "\n",
    "Misuse of joins on a count to give incorrect total - counts for a customer buying particular items\n",
    "\n",
    "How could you write a test to prevent a faulty join from happening \n",
    "\n",
    "Not using explode and instead accessing the first item of an array\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b03b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerData = [\n",
    "    (\"1\",\"James Smith\", \"07123572134\", \"james.smith1@gmail.com\"),\n",
    "    (\"2\",\"Michael Rose\", \"\", \"\"),\n",
    "    (\"3\",\"Robert Williams\", \"\", \"\"),\n",
    "    (\"4\",\"Maria Jones\", \"\", \"\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b342c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "salesDataset = [\n",
    "    \"1\", \"1\", [2,3], \"15/05/2025\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4432d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "productDataset = [\n",
    "    \"1\", \"Tennis Ball\", \"25\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab182580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType, DateType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Spark Example\").getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "random.seed = 123456\n",
    "def generate_random_digits(length=11):\n",
    "    return ''.join(str(random.randint(0, 9)) for _ in range(length))\n",
    "\n",
    "# Generate a list of 500 random strings of 11 digits\n",
    "random_digit_strings = [generate_random_digits() for _ in range(500)]\n",
    "# Generate Customer Dataset\n",
    "customer_data = []\n",
    "for i in range(1, 501):\n",
    "    customer_data.append([\n",
    "        f\"{i:03}\",\n",
    "        f\"Customer{i}\",\n",
    "        f\"{random.randint(1970, 2000)}-{random.randint(1, 12):02d}-{random.randint(1, 28):02d}\",\n",
    "        f\"{generate_random_digits()}\",\n",
    "        f\"customer{i}@example.com\"\n",
    "    ])\n",
    "\n",
    "columns = [\"CustomerId\", \"Name\", \"DoB\", \"Phone Number\", \"Email\"]\n",
    "customer_data = spark.createDataFrame(customer_data,columns )\n",
    "customer_data.write.parquet('customer_dataset.csv', mode = 'overwrite')\n",
    "\n",
    "# Generate Product Dataset\n",
    "product_data = []\n",
    "for i in range(1, 101):\n",
    "    product_data.append([\n",
    "        i,\n",
    "        f\"Product{i}\",\n",
    "        round(random.uniform(10, 100), 2)\n",
    "    ])\n",
    "\n",
    "columns = [\"ProductID\", \"Item\", \"cost\"]\n",
    "product_data = spark.createDataFrame(product_data, columns)\n",
    "product_data.write.parquet('product_dataset.parquet', mode = 'overwrite', )\n",
    "\n",
    "# Generate Sales Dataset\n",
    "sales_data = []\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "\n",
    "def random_date(start, end):\n",
    "    return start + timedelta(days=random.randint(0, (end - start).days))\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    customer_id = f\"{random.randint(1, 500):03}\"\n",
    "    num_products = random.randint(1, 5)\n",
    "    product_ids = random.sample(range(1, 101), num_products)\n",
    "    purchase_date = random_date(start_date, end_date).strftime(\"%Y-%m-%d\")\n",
    "    sales_data.append([\n",
    "        i,\n",
    "        customer_id,\n",
    "        product_ids,\n",
    "        purchase_date\n",
    "    ])\n",
    "\n",
    "# Define the schema\n",
    "sales_schema = StructType([\n",
    "    StructField(\"SaleID\", IntegerType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"ProductIDs\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"Purchase Date\", StringType(), True)  # Using StringType for simplicity\n",
    "])\n",
    "\n",
    "salesDF = spark.createDataFrame(data=sales_data,schema=sales_schema)\n",
    "salesDF.write.parquet('sales_dataset.parquet', mode = 'overwrite')\n",
    "\n",
    "print(\"CSV files generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "22d3d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = spark.read.parquet('customer_dataset.parquet', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0eb46c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+------------+--------------------+\n",
      "|CustomerId|       Name|       DoB|Phone Number|               Email|\n",
      "+----------+-----------+----------+------------+--------------------+\n",
      "|       251|Customer251|1996-07-06| 96307466380|customer251@examp...|\n",
      "|       252|Customer252|1995-06-11| 04539879250|customer252@examp...|\n",
      "|       253|Customer253|1993-05-20| 67408513309|customer253@examp...|\n",
      "|       254|Customer254|1972-05-17| 94396710154|customer254@examp...|\n",
      "|       255|Customer255|2000-09-08| 48565135257|customer255@examp...|\n",
      "|       256|Customer256|1970-08-28| 18181205096|customer256@examp...|\n",
      "|       257|Customer257|1999-04-14| 11835025584|customer257@examp...|\n",
      "|       258|Customer258|1978-09-25| 14860273878|customer258@examp...|\n",
      "|       259|Customer259|1973-06-27| 27300722658|customer259@examp...|\n",
      "|       260|Customer260|1970-10-27| 82224980518|customer260@examp...|\n",
      "|       261|Customer261|1986-07-25| 65328607930|customer261@examp...|\n",
      "|       262|Customer262|1973-11-07| 82650426693|customer262@examp...|\n",
      "|       263|Customer263|1991-04-01| 06470731088|customer263@examp...|\n",
      "|       264|Customer264|1999-06-10| 65213052039|customer264@examp...|\n",
      "|       265|Customer265|1990-03-16| 93791331361|customer265@examp...|\n",
      "|       266|Customer266|1995-10-24| 37655903380|customer266@examp...|\n",
      "|       267|Customer267|1987-05-27| 31101317084|customer267@examp...|\n",
      "|       268|Customer268|1976-02-08| 08381333007|customer268@examp...|\n",
      "|       269|Customer269|1975-11-02| 30225309517|customer269@examp...|\n",
      "|       270|Customer270|1996-11-13| 13539862135|customer270@examp...|\n",
      "+----------+-----------+----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "57715613",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = spark.read.parquet('product_dataset.parquet', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "941192ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+\n",
      "|ProductID|     Item| cost|\n",
      "+---------+---------+-----+\n",
      "|       51|Product51|50.22|\n",
      "|       52|Product52|43.18|\n",
      "|       53|Product53|12.89|\n",
      "|       54|Product54|41.46|\n",
      "|       55|Product55|44.12|\n",
      "|       56|Product56|92.95|\n",
      "|       57|Product57|90.84|\n",
      "|       58|Product58| 86.0|\n",
      "|       59|Product59|79.44|\n",
      "|       60|Product60|71.98|\n",
      "|       61|Product61|85.33|\n",
      "|       62|Product62| 50.5|\n",
      "|       63|Product63|65.69|\n",
      "|       64|Product64|95.54|\n",
      "|       65|Product65| 36.5|\n",
      "|       66|Product66|74.77|\n",
      "|       67|Product67|10.08|\n",
      "|       68|Product68|70.27|\n",
      "|       69|Product69|16.64|\n",
      "|       70|Product70|96.85|\n",
      "+---------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "acf90d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+-------------+\n",
      "|SaleID|CustomerID|          ProductIDs|Purchase Date|\n",
      "+------+----------+--------------------+-------------+\n",
      "|     1|       011|        [45, 66, 13]|   2023-02-05|\n",
      "|     2|       203|        [19, 29, 98]|   2023-07-10|\n",
      "|     3|       201|[30, 71, 87, 85, 65]|   2023-11-05|\n",
      "|     4|       044| [5, 70, 88, 81, 46]|   2023-01-27|\n",
      "|     5|       423|    [17, 26, 37, 48]|   2023-12-25|\n",
      "|     6|       130|[52, 56, 25, 93, 84]|   2023-07-27|\n",
      "|     7|       067|            [10, 75]|   2023-05-11|\n",
      "|     8|       245|                [68]|   2023-09-06|\n",
      "|     9|       373|                [89]|   2023-03-25|\n",
      "|    10|       055|                [41]|   2023-07-13|\n",
      "|    11|       137|[32, 86, 40, 45, 72]|   2023-05-15|\n",
      "|    12|       155|                [76]|   2023-09-25|\n",
      "|    13|       237|        [60, 39, 27]|   2023-10-13|\n",
      "|    14|       164|        [17, 90, 93]|   2023-11-01|\n",
      "|    15|       142|            [24, 33]|   2023-12-05|\n",
      "|    16|       315|            [93, 49]|   2023-04-30|\n",
      "|    17|       289|[40, 56, 17, 72, 99]|   2023-05-27|\n",
      "|    18|       215|[76, 60, 65, 94, 78]|   2023-07-02|\n",
      "|    19|       298|            [14, 69]|   2023-12-30|\n",
      "|    20|       313|[83, 85, 21, 37, 51]|   2023-04-30|\n",
      "+------+----------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the schema\n",
    "sales_schema = StructType([\n",
    "    StructField(\"SaleID\", IntegerType(), True),\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"ProductIDs\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"Purchase Date\", StringType(), True)  # Using StringType for simplicity\n",
    "])\n",
    "\n",
    "\n",
    "sales = spark.read.parquet('sales_dataset.parquet', schema =sales_schema,header=True)\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237dc9b",
   "metadata": {},
   "source": [
    "Read in Sales Dataset using a Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14d66e",
   "metadata": {},
   "source": [
    "Misuse of joins on a count to give incorrect total - counts for a customer buying particular items\n",
    "\n",
    "How could you write a test to prevent a faulty join from happening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a5d58a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT customerid)|\n",
      "+--------------------------+\n",
      "|                       500|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "customer.createOrReplaceTempView(\"tmp_customer\")\n",
    "sales.createOrReplaceTempView(\"tmp_sales\")\n",
    "product.createOrReplaceTempView(\"tmp_product\")\n",
    "\n",
    "#Calculate the number of customers that have signed up to the website that have made a purchase on the website\n",
    "\n",
    "# Here we are going to leave it as a left join to over generate the number of sales that there have been\n",
    "\n",
    "spark.sql(\"\"\"select count(distinct c.customerid) from tmp_customer as c left join tmp_sales as s on s.customerID = c.customerID\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3b59277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|min(customerID)|\n",
      "+---------------+\n",
      "|            001|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select min(customerID) from tmp_customer\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b13202",
   "metadata": {},
   "source": [
    "Follow up question: we want to find all the customers that haven't made a purchase yet as part of a marketing campaign, please write some code to generate that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "27df8f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         sum(cost)|\n",
      "+------------------+\n",
      "|225.35000000000002|\n",
      "| 661.8600000000001|\n",
      "|             474.5|\n",
      "|           1050.48|\n",
      "|            165.84|\n",
      "| 463.4699999999999|\n",
      "|            686.48|\n",
      "|            369.24|\n",
      "|349.71999999999997|\n",
      "| 706.6800000000001|\n",
      "|             263.5|\n",
      "|           1180.92|\n",
      "|443.34000000000003|\n",
      "| 94.32000000000001|\n",
      "|            476.64|\n",
      "|              94.4|\n",
      "|1645.4299999999998|\n",
      "| 918.9000000000001|\n",
      "|            104.37|\n",
      "|            758.88|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Here we have intentionally picked the first item in the array to give the wrong value, we are instead expecting them to try and explode this array and get the correct answer this way\n",
    "spark.sql('select sum(cost) from tmp_product as p join tmp_sales as s on element_at(s.productIDs, 1) = p.productID group by p.productID').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a72fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
